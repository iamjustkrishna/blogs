{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0407eb7c",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "## Explanation for those who just want to sound smart in a convo\n",
    "\n",
    "Basically, backpropagation is an algorithm used to efficiently compute the derivative of the loss with respect to all the weights (parameters) in a model, using the chain rule.\n",
    "\n",
    "![SMART SPECIES](../public/images/sound_smart.jpg)\n",
    "\n",
    "---\n",
    "\n",
    "## Building an Intuitive Feel (Without Losing Our Sanity)\n",
    "\n",
    "Backpropagation looks like a modern deep learning miracle, but it is actually like you, and me, just old, and ignored by our crushes for decades. All the core math behind it, the chain rule and gradient based optimization, had existed forever, sitting quietly while people chased shinier ideas. Then in the 1980s, you, backpropagation, showed up with fresh energy and a simple message, \"I can compute gradients efficiently, if you just let me.\" Papers were published, articles were written, and for a while you even made the news, but you were still mostly ignored. Finally, your era arrived. Data became massive, computers became fast, GPUs showed up, and suddenly everyone realized you had been right all along. Same math, same idea, just better timing, and no more excuses.\n",
    "\n",
    "---\n",
    "\n",
    "# Why Backpropagation?\n",
    "\n",
    "Our goal is simple: train a model that makes good predictions.\n",
    "\n",
    "Before backpropagation, neural networks did exist, but they were trained using shallow learning rules, numerical guesses, or brute force methods. None of these approaches could efficiently assign credit in deep models (models with many layers).\n",
    "\n",
    "This problem is known as the **credit assignment problem**: when the final output is wrong, which internal parameter is responsible, and by how much?\n",
    "\n",
    "In the 1970s, Paul Werbos clearly described and used backpropagation to train neural networks. However, it was only after Rumelhart, Hinton, and Williams demonstrated it clearly and practically in 1986 that the method gained attention.\n",
    "\n",
    "---\n",
    "\n",
    "## The Tea-Making Analogy (This Is the Whole Game)\n",
    "\n",
    "Imagine you are trying to make tea for your family or friends for the very first time.\n",
    "\n",
    "You do not know the recipe. You only know what **good tea tastes like**.\n",
    "\n",
    "So you start with random quantities:\n",
    "\n",
    "* some water\n",
    "* some tea leaves\n",
    "* some sugar\n",
    "\n",
    "You make the tea and ask your family to taste it.\n",
    "\n",
    "They give you a **score out of 10**, along with feedback:\n",
    "\n",
    "* \"too bitter\"\n",
    "* \"not sweet enough\"\n",
    "* \"too watery\"\n",
    "\n",
    "That score is your **loss**.\n",
    "\n",
    "You do not throw away the recipe. Instead, you adjust:\n",
    "\n",
    "* reduce tea leaves\n",
    "* add sugar\n",
    "* reduce water\n",
    "\n",
    "Then you make the tea again.\n",
    "\n",
    "Your family tastes it again, gives another score, and more feedback.\n",
    "\n",
    "You repeat this process until you consistently score something like **9 out of 10**.\n",
    "\n",
    "That loop — *make → taste → feedback → adjust* — is training.\n",
    "\n",
    "---\n",
    "\n",
    "## Mapping the Tea Analogy to Machine Learning\n",
    "\n",
    "| Tea Making                      | Machine Learning     |\n",
    "| ------------------------------- | -------------------- |\n",
    "| Ingredients (water, tea, sugar) | Parameters / weights |\n",
    "| Recipe                          | Model                |\n",
    "| Taste score                     | Loss                 |\n",
    "| Feedback                        | Gradient             |\n",
    "| Adjusting quantities            | Gradient descent     |\n",
    "| Repeating attempts              | Training loop        |\n",
    "\n",
    "The key problem is not **making tea**. The key problem is:\n",
    "\n",
    "> \"How much should I change each ingredient to improve the taste?\"\n",
    "\n",
    "Backpropagation answers exactly this question for neural networks.\n",
    "\n",
    "---\n",
    "\n",
    "## From Analogy to Math (Keeping It Minimal)\n",
    "\n",
    "We now replace tea with the simplest possible neural network.\n",
    "\n",
    "### A Tiny Model\n",
    "\n",
    "[\n",
    "\\hat{y} = wx\n",
    "]\n",
    "\n",
    "Where:\n",
    "\n",
    "* (x) is the input\n",
    "* (w) is the weight\n",
    "* (\\hat{y}) is the prediction\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "[\n",
    "L = (\\hat{y} - y)^2\n",
    "]\n",
    "\n",
    "This loss tells us how bad the prediction tastes.\n",
    "\n",
    "---\n",
    "\n",
    "## The Core Question\n",
    "\n",
    "> If I slightly change (w), how does the loss change?\n",
    "\n",
    "Mathematically, this is:\n",
    "\n",
    "[\n",
    "\\frac{\\partial L}{\\partial w}\n",
    "]\n",
    "\n",
    "Because the loss does not depend on (w) directly, we use the **chain rule**:\n",
    "\n",
    "[\n",
    "\\frac{\\partial L}{\\partial w}\n",
    "=============================\n",
    "\n",
    "\\frac{\\partial L}{\\partial \\hat{y}}\n",
    "\\cdot\n",
    "\\frac{\\partial \\hat{y}}{\\partial w}\n",
    "]\n",
    "\n",
    "This is backpropagation in its simplest form.\n",
    "\n",
    "---\n",
    "\n",
    "## What the Gradient Tells Us\n",
    "\n",
    "* **Sign**: which direction to change the weight\n",
    "* **Magnitude**: how sensitive the loss is to that weight\n",
    "\n",
    "This is exactly like adjusting sugar or tea leaves based on feedback.\n",
    "\n",
    "---\n",
    "\n",
    "## Finally, the Code (No Magic)\n",
    "\n",
    "```python\n",
    "x = 2.0\n",
    "y = 10.0\n",
    "w = 0.5\n",
    "\n",
    "# forward pass\n",
    "y_hat = w * x\n",
    "loss = (y_hat - y) ** 2\n",
    "\n",
    "# backward pass\n",
    "dL_dyhat = 2 * (y_hat - y)\n",
    "dyhat_dw = x\n",
    "dL_dw = dL_dyhat * dyhat_dw\n",
    "\n",
    "# gradient descent\n",
    "lr = 0.01\n",
    "w -= lr * dL_dw\n",
    "```\n",
    "\n",
    "Forward pass: make tea.\n",
    "\n",
    "Backward pass: get feedback.\n",
    "\n",
    "Gradient descent: adjust ingredients.\n",
    "\n",
    "Repeat until the tea tastes good.\n",
    "\n",
    "---\n",
    "\n",
    "## Final Takeaway\n",
    "\n",
    "Backpropagation is not magic. It is a systematic way to understand how each parameter contributed to the final mistake, so we can adjust them intelligently instead of guessing.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
